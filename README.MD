# CISIS-2025

This repository contains the code accompanying the paper:

**Comparing LLM-Based Query Rewriting Strategies within RAG Pipelines for Domain-Routed Legal Question Answering**

## Overview

The project implements a retrieval-augmented generation (RAG) pipeline tailored for Italian legal question answering. It evaluates three query rewriting strategies—multi-query, decomposition, and step-back—within the RAG framework. The system is designed to handle legal queries related to inheritance and divorce laws, leveraging large language models (LLMs) for query processing and answer generation.

## Repository Structure

- **`pipelines/`**: Contains implementations of the three query rewriting strategies.
- **`laws/`**: Includes the legal documents used for retrieval.
- **`evaluation/`**: Data for evaluating the performance of the different strategies.
- **`config.py`**: Configuration settings for the project.
- **`main.py`**: Entry point for running the RAG pipelines.
- **`requirements.txt`**: Lists the Python dependencies required to run the project.

## Setting Up the Environment

To set up the environment for this project, follow these steps:

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/Uzarel/CISIS-2025.git
   cd CISIS-2025
   ```

2. **Install Dependencies**:

   It's recommended to use a virtual environment to manage Python dependencies. You can create and activate a virtual environment as follows:

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

   Then, install the required packages:

   ```bash
   pip install -r requirements.txt
   ```

## Running LLMs Locally with Ollama via Docker

This project utilizes large language models (LLMs) for query processing and answer generation. To run LLMs locally, we use [Ollama](https://ollama.com/), an open-source tool that simplifies the deployment of LLMs. Below are the steps to set up and run Ollama using Docker.

### Prerequisites

- **Docker**: Ensure that Docker is installed on your system. You can download it from [here](https://www.docker.com/products/docker-desktop).

### Steps to Run Ollama with Docker

1. **Pull the Ollama Docker Image**:

   Pull the official Ollama Docker image from Docker Hub:

   ```bash
   docker pull ollama/ollama:latest
   ```

2. **Run the Ollama Container**:

   To start the Ollama container, execute the following command:

   ```bash
   docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
   ```

   *Note*: If you have an NVIDIA GPU and want to enable GPU acceleration, ensure that the NVIDIA Container Toolkit is installed. Then, run the container with GPU support:

   ```bash
   docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
   ```

3. **Pull the Desired LLM Models**:

   Once the Ollama container is running, you can pull the LLM models you intend to use. For example, to pull the Llama 3.3 model, execute:

   ```bash
   docker exec -it ollama ollama pull llama3.3
   ```

   Replace `llama3.3` with the name of the model you wish to use.

4. **Verify the Setup**:

   To verify that Ollama and the models are set up correctly, you can generate a response from the model:

   ```bash
   curl http://localhost:11434/api/generate -d '{
     "model": "llama3.3",
     "prompt": "What is the capital of Italy?"
   }'
   ```

   This should return a response generated by the Llama 3.3 model.

## Run the RAG Pipeline

Execute the main script to run the RAG pipeline:

   ```bash
   python main.py
   ```

This will process queries using the specified LLM models via Ollama.
